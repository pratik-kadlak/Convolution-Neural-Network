{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ef3VIYIyrUjbHhhmNwKAwXVCWqRYKz4f",
      "authorship_tag": "ABX9TyNvb+TATFP2rjnkny20rJnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "527832bd99704bf6806a82dfb26079db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_196a2408d1a64c3199a8794d015b7408",
              "IPY_MODEL_bd181ea547b84d72be289337b009d332",
              "IPY_MODEL_3fb27b93493a45afb30c7df988ece7ac"
            ],
            "layout": "IPY_MODEL_b59c5843aeda412f879fbb10dd9ebe8a"
          }
        },
        "196a2408d1a64c3199a8794d015b7408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e33023082da4d8b8e87765c8accb766",
            "placeholder": "​",
            "style": "IPY_MODEL_ff7ee6587b7446598b5a856ae7235301",
            "value": " 55%"
          }
        },
        "bd181ea547b84d72be289337b009d332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f63b64665e842f082b026462865ff36",
            "max": 315,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee2aab3f223d42668862e6261fdf75a4",
            "value": 172
          }
        },
        "3fb27b93493a45afb30c7df988ece7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565e95ab4dd74326923200c7fe6cfbc4",
            "placeholder": "​",
            "style": "IPY_MODEL_5369ca28b9534750b4ca0c873a1f4fe6",
            "value": " 172/315 [19:58&lt;18:32,  7.78s/it]"
          }
        },
        "b59c5843aeda412f879fbb10dd9ebe8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e33023082da4d8b8e87765c8accb766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff7ee6587b7446598b5a856ae7235301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f63b64665e842f082b026462865ff36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee2aab3f223d42668862e6261fdf75a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "565e95ab4dd74326923200c7fe6cfbc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5369ca28b9534750b4ca0c873a1f4fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratik-kadlak/Convolution-Neural-Network/blob/main/dl_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfQtHdSXCYwM"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(image_matrix):\n",
        "    \"\"\"\n",
        "    Display an image using Matplotlib.\n",
        "\n",
        "    Parameters:\n",
        "    - image_matrix (numpy.ndarray): NumPy array containing the image data.\n",
        "                                    Should be in the format (channels, height, width).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    image = image_matrix\n",
        "    # Transpose the image array from (channels, height, width) to (height, width, channels) for Matplotlib\n",
        "    image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "    # Display the image using Matplotlib\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')  # Turn off axis labels\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qixFUnl0Cf03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_images(path):\n",
        "    data_transform = transforms.Compose([transforms.Resize((227,227)), transforms.ToTensor()])\n",
        "    dataset = ImageFolder(path, transform=data_transform)\n",
        "\n",
        "    data = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for image, label in tqdm(data):\n",
        "        X.append(image)\n",
        "        y.append(label)\n",
        "\n",
        "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
        "    X = np.concatenate(X, axis=0)\n",
        "    y = np.concatenate(y, axis=0)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "g7uUrtHiD6t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(train_size:float = 0.2):\n",
        "    \"\"\"\n",
        "    Split the training data into training and validation sets.\n",
        "\n",
        "    Parameters:\n",
        "    - train_size (float, optional): The proportion of the dataset to include in the validation set (default=0.2).\n",
        "\n",
        "    Returns:\n",
        "    - X_train (numpy.ndarray): NumPy array containing training images.\n",
        "    - X_val (numpy.ndarray): NumPy array containing validation images.\n",
        "    - y_train (numpy.ndarray): NumPy array containing training labels.\n",
        "    - y_val (numpy.ndarray): NumPy array containing validation labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize lists to store validation and training data\n",
        "    X_val = []\n",
        "    y_val = []\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # 1000 because we have 1000 images of each class\n",
        "    samples_per_class_val = (int) (1000 * train_size)\n",
        "\n",
        "    for class_label in range(10):\n",
        "        # extract indices corresponding to the current class\n",
        "        class_indices = np.where(y_train==class_label)[0]\n",
        "\n",
        "        # randomly select sample_per_class_val indices for validation\n",
        "        val_indices = np.random.choice(class_indices, samples_per_class_val, replace=False)\n",
        "\n",
        "        # append the selected val data to X_val and y_val\n",
        "        X_val.extend(X_train[val_indices])\n",
        "        y_val.extend(y_train[val_indices])\n",
        "\n",
        "        # append the remaining data to X_train and y_train\n",
        "        train_indices = np.setdiff1d(class_indices, val_indices)\n",
        "        X.extend(X_train[train_indices])\n",
        "        y.extend(y_train[train_indices])\n",
        "\n",
        "    # convert python lists to np array\n",
        "    X_val = np.array(X_val)\n",
        "    y_val = np.array(y_val)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, X_val, y, y_val"
      ],
      "metadata": {
        "id": "Q3FxrQToD_Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_data(X, y):\n",
        "    \"\"\"\n",
        "    Shuffle data samples and their corresponding labels.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): NumPy array containing data samples.\n",
        "    - y (numpy.ndarray): NumPy array containing corresponding labels.\n",
        "\n",
        "    Returns:\n",
        "    - X_shuffled (numpy.ndarray): NumPy array containing shuffled data samples.\n",
        "    - y_shuffled (numpy.ndarray): NumPy array containing corresponding shuffled labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # Combine X, y into a list of tuples\n",
        "    data = list(zip(X, y))\n",
        "\n",
        "    # Shuffle the combined data\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # Unpack the shuffled data back into separate arrays\n",
        "    X_shuffled, y_shuffled = zip(*data)\n",
        "\n",
        "    # Convert the shuffled lists to NumPy arrays\n",
        "    X_shuffled = np.array(X_shuffled)\n",
        "    y_shuffled = np.array(y_shuffled)\n",
        "\n",
        "    return X_shuffled, y_shuffled"
      ],
      "metadata": {
        "id": "a2YZcXXPEDz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(X, y, batch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Create a PyTorch DataLoader from input data and labels.\n",
        "\n",
        "    Parameters:\n",
        "    - X (numpy.ndarray): Input data array.\n",
        "    - y (numpy.ndarray): Labels array.\n",
        "    - batch_size (int, optional): Batch size for DataLoader (default=32).\n",
        "    - shuffle (bool, optional): Whether to shuffle the data (default=True).\n",
        "\n",
        "    Returns:\n",
        "    - DataLoader: PyTorch DataLoader for the input data and labels.\n",
        "    \"\"\"\n",
        "    # Convert NumPy arrays to PyTorch tensors\n",
        "    X_tensor = torch.from_numpy(X)\n",
        "    y_tensor = torch.from_numpy(y)\n",
        "\n",
        "    # Create a TensorDataset from X_train_tensor and y_train_tensor\n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "    # Define batch size and create DataLoader\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return loader"
      ],
      "metadata": {
        "id": "0_NhR6foEG3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data():\n",
        "    # Define data augmentation transformations\n",
        "    augmented_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "        transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, saturation, and hue\n",
        "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    ])\n",
        "\n",
        "    # Apply data augmentation to the original dataset\n",
        "    augmented_dataset = ConcatDataset([train_loader.dataset, train_loader.dataset])\n",
        "\n",
        "    # Create a DataLoader for the combined dataset\n",
        "    combined_loader = DataLoader(augmented_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
        "\n",
        "    return combined_loader"
      ],
      "metadata": {
        "id": "-F9xeaIsEWf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct/len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Rsy0vV7aEbwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def train_step(model:torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "\n",
        "    train_loss, train_acc = 0,0\n",
        "    model.train()\n",
        "\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # put data on target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # calc loss (per batch) and accuracy\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # back pass\n",
        "        loss.backward()\n",
        "\n",
        "        # updating the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "kfeReYbkEiRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in data_loader:\n",
        "            # send data to the target device\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            test_pred = model(X_test)\n",
        "\n",
        "            # calc loss\n",
        "            test_loss += loss_fn(test_pred, y_test)\n",
        "\n",
        "            # calc add\n",
        "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # clac the test loss avg per batch\n",
        "        test_loss /= len(data_loader)\n",
        "\n",
        "        # calc the test acc avg per batch\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "        return test_loss, test_acc"
      ],
      "metadata": {
        "id": "ggWJnl_1Estt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    fc_input = 1\n",
        "    def __init__(self, in_channels, out_channels, num_filters, kernel_size, activation_fn, apply_batchnorm, apply_dropout, prob, hidden_units):\n",
        "        super(CNN, self).__init__()\n",
        "        # Define the convolution layers\n",
        "        self.conv1 = nn.Conv2d(\n",
        "                               in_channels=in_channels,\n",
        "                               out_channels=num_filters[0],\n",
        "                               kernel_size=kernel_size[0],\n",
        "                               stride=1,\n",
        "                               padding=0\n",
        "                              )\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_filters[0])\n",
        "        self.conv2 = nn.Conv2d(\n",
        "                               in_channels=num_filters[0],\n",
        "                               out_channels=num_filters[1],\n",
        "                               kernel_size=kernel_size[1],\n",
        "                               stride=1,\n",
        "                               padding=0\n",
        "                              )\n",
        "        self.batchnorm2 = nn.BatchNorm2d(num_filters[1])\n",
        "        self.conv3 = nn.Conv2d(\n",
        "                               in_channels=num_filters[1],\n",
        "                               out_channels=num_filters[2],\n",
        "                               kernel_size=kernel_size[2],\n",
        "                               stride=1,\n",
        "                               padding=0\n",
        "                              )\n",
        "        self.batchnorm3 = nn.BatchNorm2d(num_filters[2])\n",
        "        self.conv4 = nn.Conv2d(\n",
        "                               in_channels=num_filters[2],\n",
        "                               out_channels=num_filters[3],\n",
        "                               kernel_size=kernel_size[3],\n",
        "                               stride=1,\n",
        "                               padding=0\n",
        "                              )\n",
        "        self.batchnorm4 = nn.BatchNorm2d(num_filters[3])\n",
        "        self.conv5 = nn.Conv2d(\n",
        "                               in_channels=num_filters[3],\n",
        "                               out_channels=num_filters[4],\n",
        "                               kernel_size=kernel_size[4],\n",
        "                               stride=1,\n",
        "                               padding=0\n",
        "                              )\n",
        "        self.batchnorm5 = nn.BatchNorm2d(num_filters[4])\n",
        "\n",
        "        # Define activation function\n",
        "        if activation_fn == \"ReLU\": self.activation = nn.ReLU()\n",
        "        elif activation_fn == \"GELU\": self.activation = nn.GELU()\n",
        "        elif activation_fn == \"SiLU\": self.activation = nn.SiLU()\n",
        "        elif activation_fn == \"Mish\": self.activation = nn.Mish()\n",
        "\n",
        "        # Define max pooling layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        input_size = 227\n",
        "        for i in range(5):\n",
        "            input_size = input_size - kernel_size[i] + 1\n",
        "            input_size = input_size // 2\n",
        "\n",
        "        # Define dense layer\n",
        "        self.fc1 = nn.Linear(input_size*input_size*num_filters[4], hidden_units)\n",
        "\n",
        "        # Adding Dropout\n",
        "        self.dropout = nn.Dropout(p=prob)\n",
        "\n",
        "        # Define output layer\n",
        "        self.fc2 = nn.Linear(hidden_units, 10)  # 10 output neurons for 10 classes\n",
        "\n",
        "        self.apply_batchnorm = apply_batchnorm\n",
        "        self.apply_dropout = apply_dropout\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolution, activation, and max pooling layers\n",
        "        x = self.pool(self.activation(self.batchnorm1(self.conv1(x)) if self.apply_batchnorm ==\"Yes\" else self.conv1(x)))\n",
        "        x = self.pool(self.activation(self.batchnorm2(self.conv2(x)) if self.apply_batchnorm ==\"Yes\" else self.conv2(x)))\n",
        "        x = self.pool(self.activation(self.batchnorm3(self.conv3(x)) if self.apply_batchnorm ==\"Yes\" else self.conv3(x)))\n",
        "        x = self.pool(self.activation(self.batchnorm4(self.conv4(x)) if self.apply_batchnorm ==\"Yes\" else self.conv4(x)))\n",
        "        x = self.pool(self.activation(self.batchnorm5(self.conv5(x)) if self.apply_batchnorm ==\"Yes\" else self.conv5(x)))\n",
        "\n",
        "        # Flatten the output for the dense layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply dense layer and output layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        if self.apply_dropout==\"Yes\": x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)  # Apply softmax activation to the output\n",
        "        return x"
      ],
      "metadata": {
        "id": "WvzYtS1OE53Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/kaggle/input/inaturalist/inaturalist_12K/train\"\n",
        "test_path = \"/kaggle/input/inaturalist/inaturalist_12K/val\"\n",
        "\n",
        "X_train, y_train = read_images(train_path)\n",
        "X_test, y_test = read_images(test_path)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_val_split(0.2)\n",
        "\n",
        "X_train, y_train = shuffle_data(X_train, y_train)\n",
        "X_val, y_val = shuffle_data(X_val, y_val)\n",
        "\n",
        "train_loader = create_dataloader(X_train, y_train, 32)\n",
        "val_loader = create_dataloader(X_val, y_val, 32)\n",
        "test_loader = create_dataloader(X_test, y_test, 32)"
      ],
      "metadata": {
        "id": "MWGbx3UkE-lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397,
          "referenced_widgets": [
            "527832bd99704bf6806a82dfb26079db",
            "196a2408d1a64c3199a8794d015b7408",
            "bd181ea547b84d72be289337b009d332",
            "3fb27b93493a45afb30c7df988ece7ac",
            "b59c5843aeda412f879fbb10dd9ebe8a",
            "7e33023082da4d8b8e87765c8accb766",
            "ff7ee6587b7446598b5a856ae7235301",
            "4f63b64665e842f082b026462865ff36",
            "ee2aab3f223d42668862e6261fdf75a4",
            "565e95ab4dd74326923200c7fe6cfbc4",
            "5369ca28b9534750b4ca0c873a1f4fe6"
          ]
        },
        "outputId": "85678e93-f3d7-41e5-e4f1-a29223c3ba66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/315 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "527832bd99704bf6806a82dfb26079db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-baf6643b6928>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"//content/drive/MyDrive/DeepLearning-IITM/Assignment_2/inaturalist_12K/val\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-8e3989e2ba8f>\u001b[0m in \u001b[0;36mread_images\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "\"name\": \"CNN\",\n",
        "\"metric\": {\n",
        "    \"name\":\"val_accuracy\",\n",
        "    \"goal\": \"maximize\"\n",
        "},\n",
        "\"method\": \"bayes\",\n",
        "\"parameters\": {\n",
        "        \"epoch\": {\n",
        "            \"values\": [5,10]\n",
        "        },\n",
        "        \"num_filters\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"activation_func\": {\n",
        "            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
        "        },\n",
        "        \"filter_org\": {\n",
        "            \"values\": [\"same\", \"half\", \"double\"]\n",
        "        },\n",
        "        \"data_augment\": {\n",
        "            \"values\": [\"Yes\", \"No\"]\n",
        "        },\n",
        "        \"batch_normalization\": {\n",
        "            \"values\": [\"Yes\", \"No\"]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [\"Yes\", \"No\"]\n",
        "        },\n",
        "        \"prob\": {\n",
        "            \"values\": [0.2, 0.3]\n",
        "        },\n",
        "        \"filter_size\": {\n",
        "            \"values\": [[3,3,3,3,3], [4,4,4,4,4], [5,5,5,5,5]]\n",
        "        },\n",
        "        \"hidden_units\": {\n",
        "            \"values\": [128, 256]\n",
        "        },\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "QSolPZXqclcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cnn(train_loader, val_loader, config):\n",
        "    epochs = config.epoch\n",
        "    in_channels = 3\n",
        "    out_channels = 10\n",
        "    num_filters = [config.num_filters]\n",
        "    kernel_size = config.filter_size\n",
        "    activation_fn = config.activation_func\n",
        "    augment = config.data_augment\n",
        "    filter_org = config.filter_org\n",
        "    batch_norm = config.batch_normalization\n",
        "    dropout = config.dropout\n",
        "    prob = config.prob\n",
        "    hidden_units = config.hidden_units\n",
        "\n",
        "    for i in range(4):\n",
        "        last_value = num_filters[-1]\n",
        "        if(filter_org == \"same\"): num_filters.append(last_value)\n",
        "        elif(filter_org == \"half\"): num_filters.append((int)(last_value * 0.5))\n",
        "        else: num_filters.append(last_value * 2)\n",
        "\n",
        "    if augment == \"Yes\":\n",
        "        train_loader = augment_data()\n",
        "\n",
        "    run_name = f\"epoch_{epochs}_num_filters_{num_filters[0]}_act_{activation_fn}_filt_org_{filter_org}_augment_{augment}_batchnorm_{batch_norm}_dropout_{dropout}_prob_{prob}_hu_{hidden_units}\"\n",
        "\n",
        "    model = CNN(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                num_filters=num_filters,\n",
        "                kernel_size=kernel_size,\n",
        "                activation_fn=activation_fn,\n",
        "                apply_batchnorm=batch_norm,\n",
        "                apply_dropout=dropout,\n",
        "                prob=prob,\n",
        "                hidden_units=hidden_units\n",
        "            )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # setting up Loss and Optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
        "    # optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        print(f\"Epoch: {epoch}\\n------\")\n",
        "        train_loss, train_accuracy = train_step(\n",
        "                                         model=model,\n",
        "                                         data_loader=train_loader,\n",
        "                                         loss_fn=loss_fn,\n",
        "                                         optimizer=optimizer,\n",
        "                                         accuracy_fn=accuracy_fn,\n",
        "                                         device=device\n",
        "                                     )\n",
        "\n",
        "        print(f\"Train Loss: {train_loss: .5f} | Train Acc: {train_accuracy: .2f}%\")\n",
        "\n",
        "        val_loss, val_accuracy = test_step(\n",
        "                                    model=model,\n",
        "                                    data_loader=val_loader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    accuracy_fn=accuracy_fn,\n",
        "                                    device=device\n",
        "                                )\n",
        "\n",
        "\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "        wandb.log({\"val_accuracy\":val_accuracy, 'val_loss':val_loss, 'train_accuracy':train_accuracy, 'train_loss':train_loss})\n",
        "\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "    wandb.run.finish()"
      ],
      "metadata": {
        "id": "9NT5yp4scuiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    with wandb.init(project=\"DL_Assignment_2\") as run:\n",
        "        config = wandb.config\n",
        "        train_cnn(train_loader, val_loader, config)\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"DL_Assignment_2\")\n",
        "wandb.agent(sweep_id, train, count = 20)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "ob7xLyG1cu5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gxzY97-cvSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}